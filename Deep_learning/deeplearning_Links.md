# Cousera GCP:

https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/06_structured/3_tensorflow_dnn.ipynb


# TF tutorials links

https://data-flair.training/blogs/mandelbrot-set-in-tensorflow/


# Deep learning indian course
https://www.cse.iitm.ac.in/~miteshk/CS7015.html

https://www.youtube.com/watch?v=ewN0vFYFJ7A&list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&index=37


# Loss_function:
https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0

https://medium.com/@risingdeveloper/visualization-of-some-loss-functions-for-deep-learning-with-tensorflow-9f60be9d09f9

https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html


# L1 and L2 loss:
http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/

# L2 Regulisation adding to the cost function:

https://github.com/keras-team/keras/issues/2104

# Regulisation in Neural network TensorflowL

https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/

# cross entropy:
https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8


# Multi-Class Multi-Label Classification With Neural Networks:
https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks

#  Optimizer functions in neural network:
http://ruder.io/optimizing-gradient-descent/

http://cs231n.github.io/neural-networks-3/

# Batch Normalization :

https://blog.paperspace.com/busting-the-myths-about-batch-normalization/

https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c

http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/ 

https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85

# Batch ReNormalization :
https://medium.com/luminovo/a-refresher-on-batch-re-normalization-5e0a1e902960

https://www.quora.com/Could-Batch-Renormalization-replace-Virtual-Batch-Normalization-in-GANs



# Local Minima problem:

https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/


# Vanishing gradients:

https://blog.paperspace.com/vanishing-gradients-activation-function/


# Momentum:
https://distill.pub/2017/momentum/


# Gradient Checkpoint:
https://github.com/openai/gradient-checkpointing

# RNN:
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/

http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

# LSTM

https://www.coursera.org/lecture/ai/number-of-parameters-A3N10

https://www.quora.com/How-is-the-hidden-state-h-different-from-the-memory-c-in-an-LSTM-cell


# attention model LSTM

https://hergott.github.io/lstm-attention-bond-market-taper-tantrum/

https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c


# CNN Time series:
http://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-1

http://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-2

http://blog.avenuecode.com/using-deep-convolutional-neural-networks-dcnns-for-time-series-forecasting-using-tensorflow-part-3



# goldilocks learning rate:



# Difference between SGD,  Gradient decent and mini batch gradient decent:

https://datascience.stackexchange.com/questions/53870/how-do-gd-batch-gd-sgd-and-mini-batch-sgd-differ/53915

